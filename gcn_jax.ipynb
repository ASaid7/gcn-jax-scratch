{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc0031d-6479-4468-90e6-85bfe54e95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asaid/geomtry/lib/python3.8/site-packages/jax/experimental/optimizers.py:28: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
      "  warnings.warn('jax.experimental.optimizers is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "from jax.nn.initializers import glorot_normal, zeros\n",
    "from jax.experimental import optimizers\n",
    "from jax import value_and_grad, grad\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db33c0-074c-4263-8a20-df767a5fd7b4",
   "metadata": {},
   "source": [
    "## Implementation of GCN in JAX\n",
    "#### GCN Layer is defined as:\n",
    "#### $H^{(l+1)}$ = $\\sigma$($\\hat A$ ⋅ $H^{(l)}$ ⋅ $W^{(l)}$)\n",
    "#### Where l layer, $\\sigma$ is an activation function, and $W^{(l)}$ layer wise trainable weight matrix\n",
    "#### $\\hat A$ = $\\tilde D^{-1/2}$ ⋅ $\\tilde A$ ⋅ $\\tilde D^{-1/2}$\n",
    "#### $\\tilde A$ = I + A\n",
    "#### $\\tilde D$ = diag($\\tilde A$)\n",
    "#### $H^0$ = X\n",
    "\n",
    "GCN uses both node features and adjcancy matrix giving it the ability to capture both node features and graph structure in its node classification.\n",
    "Please read the paper for more details https://arxiv.org/abs/1609.02907v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1efbac7-c806-41f6-802a-ac0d287689f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer_dim, seed = 7):\n",
    "        k0, k1 = jax.random.split(jax.random.PRNGKey(seed))\n",
    "        params = []\n",
    "        for _in, _out in zip(layer_dim[:-1], layer_dim[1:]):\n",
    "            w = glorot_normal()(k0,(_in,_out))\n",
    "            b = zeros(k0,(_out,))\n",
    "            params.append((w,b))\n",
    "        return params\n",
    "\n",
    "def gcn_layer(A, X, w, b):\n",
    "        h = A @ X @ w\n",
    "        h += b\n",
    "        return h\n",
    "\n",
    "def forward_pass(A, X, params):\n",
    "    h = X\n",
    "    for w, b in params[:-1]:\n",
    "        h = nn.relu(gcn_layer(A, h, w, b))\n",
    "    w, b = params[-1]\n",
    "    h = gcn_layer(A, h, w, b)\n",
    "    out = nn.softmax(h)\n",
    "    return out\n",
    "\n",
    "def loss_criteria(params,A,X,y, mask):\n",
    "    logit = forward_pass(A, X, params)\n",
    "    if mask is not None:\n",
    "        logit = logit[mask,]\n",
    "        y = y[mask,]\n",
    "    m = y.shape[0]\n",
    "    cost = -(1/m) * jnp.sum(y*jnp.log(logit))\n",
    "    return cost\n",
    "\n",
    "def update(params,A,X,y,opt_state, epoch, mask=None):\n",
    "    loss, gradient = value_and_grad(loss_criteria)(params,A,X,y,mask)\n",
    "    opt_state = opt_update(epoch, gradient, opt_state)\n",
    "    return get_params(opt_state), opt_state, loss\n",
    "\n",
    "def accuracy(y,yhat):\n",
    "    return sum(y==yhat)/y.shape[0]\n",
    "\n",
    "def ahat(A):\n",
    "    I = jnp.identity(A.shape[0])\n",
    "    A_tilda = A + I\n",
    "    D_tilda = jnp.zeros(A_tilda.shape)\n",
    "    D_tilda = fill_diagonal(D_tilda, A_tilda.sum(axis=1).flatten())\n",
    "    D_tilda_inv_sqrt = jnp.linalg.inv(jnp.sqrt(D_tilda))\n",
    "    A_hat = D_tilda_inv_sqrt @ A_tilda @ D_tilda_inv_sqrt\n",
    "    return A_hat\n",
    "    \n",
    "def fill_diagonal(A, val):\n",
    "    assert A.ndim >= 2\n",
    "    i, j = jnp.diag_indices(min(A.shape[-2:]))\n",
    "    return A.at[..., i, j].set(val)\n",
    "\n",
    "def train_test_idx(n,p,seed=42):\n",
    "    k0, k1 = jax.random.split(jax.random.PRNGKey(seed))\n",
    "    idx = jnp.linspace(0,n,n-1,dtype=int)\n",
    "    idx = jax.random.shuffle(k0,idx)\n",
    "    e = int(n*p)\n",
    "    return idx[:e],idx[e:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8470cb19-5337-41a9-bbca-8f640aa31b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/asaid/geomtry/lib/python3.8/site-packages/jax/_src/random.py:369: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cora.content',sep='\\t',header=None)\n",
    "df.set_index(0,inplace=True)\n",
    "\n",
    "y,decode = jnp.array(df.iloc[:,-1].astype('category').cat.codes), df.iloc[:,-1]\n",
    "label_dict = {label:code for label, code in  zip(y, decode)}\n",
    "X = jnp.array(df.iloc[:,:-1])\n",
    "\n",
    "g = nx.read_edgelist('cora.cites', create_using=nx.Graph(), nodetype=int)\n",
    "A = jnp.array(nx.to_numpy_matrix(g,nodelist = df.index))\n",
    "\n",
    "#Train Model with only 5% of the labels; gradient wont be passed to nodes outside of the train_idx\n",
    "train_idx, val_idx = train_test_idx(2708,.05,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c7b451-1e82-4bce-bd84-5baa036e0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1433,128,7]\n",
    "lr = 1e-4\n",
    "epochs = 450\n",
    "\n",
    "a_hat = ahat(A)\n",
    "y_ohe = nn.one_hot(y, 7)\n",
    "init_params = init_weights(layers)\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "opt_state = opt_init(init_params)\n",
    "params = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701af365-5fda-428a-be0f-8761b1de6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.9420676231384277\n",
      "Val Acc: 0.1916796267496112\n",
      "Training Loss: 1.8362723588943481\n",
      "Val Acc: 0.35303265940902023\n",
      "Training Loss: 1.7304749488830566\n",
      "Val Acc: 0.37636080870917576\n",
      "Training Loss: 1.6153335571289062\n",
      "Val Acc: 0.40552099533437014\n",
      "Training Loss: 1.4892604351043701\n",
      "Val Acc: 0.48755832037325036\n",
      "Training Loss: 1.3552714586257935\n",
      "Val Acc: 0.5579315707620529\n",
      "Training Loss: 1.2192163467407227\n",
      "Val Acc: 0.619751166407465\n",
      "Training Loss: 1.0876827239990234\n",
      "Val Acc: 0.6527993779160186\n",
      "Training Loss: 0.9655272364616394\n",
      "Val Acc: 0.6811819595645412\n",
      "Training Loss: 0.8549657464027405\n",
      "Val Acc: 0.7002332814930016\n",
      "Training Loss: 0.7557722330093384\n",
      "Val Acc: 0.7173405909797823\n",
      "Training Loss: 0.6680740714073181\n",
      "Val Acc: 0.7282270606531882\n",
      "Training Loss: 0.5909103155136108\n",
      "Val Acc: 0.7422239502332815\n",
      "Training Loss: 0.5231021046638489\n",
      "Val Acc: 0.7507776049766719\n",
      "Training Loss: 0.46374934911727905\n",
      "Val Acc: 0.7569984447900466\n",
      "Training Loss: 0.41214001178741455\n",
      "Val Acc: 0.7601088646967341\n",
      "Training Loss: 0.36734235286712646\n",
      "Val Acc: 0.7643856920684292\n",
      "Training Loss: 0.3283490240573883\n",
      "Val Acc: 0.7667185069984448\n",
      "Training Loss: 0.29443594813346863\n",
      "Val Acc: 0.7702177293934681\n",
      "Training Loss: 0.26489895582199097\n",
      "Val Acc: 0.77099533437014\n",
      "Training Loss: 0.2391420155763626\n",
      "Val Acc: 0.7752721617418351\n",
      "Training Loss: 0.2166067510843277\n",
      "Val Acc: 0.776438569206843\n",
      "Training Loss: 0.19682809710502625\n",
      "Val Acc: 0.7787713841368584\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    params, opt_state, loss = update(params,a_hat,X,y_ohe,opt_state, i, train_idx)\n",
    "    if i % 20 == 0:\n",
    "        print(f'Training Loss: {loss}')\n",
    "        yhat = jnp.argmax(forward_pass(a_hat, X, params),axis=1)\n",
    "        acc = accuracy(y[val_idx,],yhat[val_idx,])\n",
    "        print(f'Val Acc: {acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cda591-03e8-4fc7-bd07-eeb42e61f963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
